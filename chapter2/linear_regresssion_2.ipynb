{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等。\n",
    "\n",
    "TensorDataset 中的参数必须是 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[11, 22, 33],\n",
      "        [44, 55, 66],\n",
      "        [77, 88, 99]]), tensor([0, 1, 2]))\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "a = torch.tensor([[11, 22, 33], [44, 55, 66], [77, 88, 99], [11, 22, 33], [44, 55, 66], [77, 88, 99], [11, 22, 33], [44, 55, 66], [77, 88, 99], [11, 22, 33], [44, 55, 66], [77, 88, 99]])\n",
    "b = torch.tensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])\n",
    "train_ids = TensorDataset(a, b)\n",
    "# 切片输出\n",
    "print(train_ids[:3])\n",
    "print('#' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 22, 33]) tensor(0)\n",
      "tensor([44, 55, 66]) tensor(1)\n",
      "tensor([77, 88, 99]) tensor(2)\n",
      "tensor([11, 22, 33]) tensor(0)\n",
      "tensor([44, 55, 66]) tensor(1)\n",
      "tensor([77, 88, 99]) tensor(2)\n",
      "tensor([11, 22, 33]) tensor(0)\n",
      "tensor([44, 55, 66]) tensor(1)\n",
      "tensor([77, 88, 99]) tensor(2)\n",
      "tensor([11, 22, 33]) tensor(0)\n",
      "tensor([44, 55, 66]) tensor(1)\n",
      "tensor([77, 88, 99]) tensor(2)\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 循环取数据\n",
    "for x_train, y_label in train_ids:\n",
    "    print(x_train, y_label)\n",
    "# DataLoader进行数据封装\n",
    "print('#' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch:1\n",
      " x_data:\n",
      "tensor([[44, 55, 66],\n",
      "        [44, 55, 66],\n",
      "        [77, 88, 99],\n",
      "        [11, 22, 33]])  label: \n",
      "tensor([1, 1, 2, 0])\n",
      " batch:2\n",
      " x_data:\n",
      "tensor([[11, 22, 33],\n",
      "        [44, 55, 66],\n",
      "        [77, 88, 99],\n",
      "        [77, 88, 99]])  label: \n",
      "tensor([0, 1, 2, 2])\n",
      " batch:3\n",
      " x_data:\n",
      "tensor([[11, 22, 33],\n",
      "        [44, 55, 66],\n",
      "        [11, 22, 33],\n",
      "        [77, 88, 99]])  label: \n",
      "tensor([0, 1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# 每次挑选batch_size个数据集出来\n",
    "train_loader = DataLoader(dataset=train_ids, batch_size=4, shuffle=True)\n",
    "for i, data in enumerate(train_loader, 1):  # 注意enumerate返回值有两个,一个是序号，一个是数据（包含训练数据和标签）\n",
    "    x_data, label = data\n",
    "    print(' batch:{0}\\n x_data:\\n{1}  label: \\n{2}'.format(i, x_data, label))   # y data (torch tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random  # 随机梯度下降和随机初始化权重\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import  torch as d2l\n",
    "\n",
    "\n",
    "# 构造数据集\n",
    "def generate_data(w, b, examples_num):\n",
    "    # Y = X*w + b + noise\n",
    "    X = torch.normal(0, 1,(examples_num, len(w)))  # 最后输出一个examples_num * len（w）的样本tensor\n",
    "    # print( \" X \", X)\n",
    "    # torch.matmul()若两个tensor都是一维的，则返回两个向量的点积运算结果\n",
    "    Y = torch.matmul(X,w) + b\n",
    "    # print( \" Y \", Y)\n",
    "    Y += torch.normal(0, 0.01, Y.shape)\n",
    "    return X, Y.reshape((-1, 1))  # 把Y做成一个列向量返回\n",
    "\n",
    "true_w = torch.tensor([1.5, 2.8])\n",
    "true_b = 3.33\n",
    "features, labels = generate_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.6787,  0.2679],\n",
       "         [ 1.4045,  0.1209],\n",
       "         [ 0.6498,  1.5793],\n",
       "         [-0.0988,  2.0127],\n",
       "         [-0.8026,  0.8092],\n",
       "         [-1.8287,  2.2483],\n",
       "         [ 2.2411, -0.8750],\n",
       "         [ 0.5960,  0.8445],\n",
       "         [-0.9508, -0.0489],\n",
       "         [ 0.9945,  1.6306]]),\n",
       " tensor([[1.5592],\n",
       "         [5.7780],\n",
       "         [8.7178],\n",
       "         [8.8166],\n",
       "         [4.3966],\n",
       "         [6.9010],\n",
       "         [4.2429],\n",
       "         [6.5917],\n",
       "         [1.7719],\n",
       "         [9.3749]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(data_arrays, batch_size, is_train = True):\n",
    "    \"\"\"构造一个pytorch的数据迭代器\n",
    "\n",
    "    Args:\n",
    "        data_arrays (_type_): _description_\n",
    "        batch_size (_type_): _description_\n",
    "        is_train (bool, optional): _description_. Defaults to True.\n",
    "    \"\"\"\n",
    "    data_set = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(data_set, batch_size, shuffle = is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_data((features, labels), batch_size)\n",
    "\n",
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型 \"nn\"是神经网络的缩写\n",
    "from torch import nn\n",
    "\n",
    "# nn.Sequential =  list of layers\n",
    "net = nn.Sequential(nn.Linear(2,1))  # 输入维度是2 输出维度是1\n",
    "\n",
    "# net[0] 访问 nn.Sequential(nn.Linear(2,1)) 的第0层\n",
    "# .weight 访问 权重 W\n",
    "# .data 就是 W 的真实data\n",
    "# normal_(0, 0.01)用（0， 0.01）替换data中的值\n",
    "# 相当于之前实现 w b\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "# 相当于之前的network偏差设为0\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "# net.parameters 包括 w 和 b\n",
    "trainer = torch.optim.SGD(net.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000098\n",
      "epoch 2, loss 0.000097\n",
      "epoch 3, loss 0.000097\n"
     ]
    }
   ],
   "source": [
    "epochs_num = 3\n",
    "for epoch in range(epochs_num):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        # 优化器先把梯度清零\n",
    "        trainer.zero_grad()\n",
    "        # pytorch 做了 sum\n",
    "        l.backward()\n",
    "        # 进行一次模型的更新\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    # {l:f} 以浮点型的格式打印l\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('limu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a044d7688e486ae77cd164cc30ce7e14bb396943c06b871a0658b418ee8b750"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
